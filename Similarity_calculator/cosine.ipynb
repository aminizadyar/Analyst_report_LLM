{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not-summarized earnings calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read text from all four files\n",
    "files = ['1/KB Home, Q3 2009 Earnings Call, Sep-25-2009.txt'\n",
    "         ,'1/KB Home earnings transcript summary.txt' \n",
    "         , '1/claude_earnings_calls_KBH.txt'\n",
    "         , '1/clean_Report_JPMORGAN_2009-09-25_Page_2_Doc_3.txt'\n",
    "         , '1/claude_analyst_report.txt'\n",
    "         , '2/McCormick & Co. Inc., Q3 2009 Earnings Call, Sep-24-2009 (1).txt'\n",
    "         , '2/McCormic call summary.txt'\n",
    "         , '2/claude_earnings_call_mccormick.txt'\n",
    "         , '2/clean_Report_JPMORGAN_2009-09-25_Page_1_Doc_37.txt'\n",
    "         , '2/claude_clean_Report_JPMORGAN_2009-09-25_Page_1_Doc_37.txt'\n",
    "         , '3/PPG Industries, Inc., Q3 2018 Earnings Call, Oct 18, 2018.txt'\n",
    "         , '3/ppg earnings summary.txt'\n",
    "         , '3/clean_Report_BARCLAYS_2018-10-22_Page_1_Doc_17.txt'\n",
    "         , '4/Lennox International Inc., Q3 2018 Earnings Call, Oct 22, 2018.txt'\n",
    "         , '4/lenox earnings summary.txt'\n",
    "         , '4/clean_Report_BARCLAYS_2018-10-22_Page_1_Doc_11.txt'\n",
    "         , '5/IQVIA Holdings Inc., Q3 2018 Earnings Call, Oct 22, 2018.txt'\n",
    "         , '5/IQVIA earnings summary.txt'\n",
    "         , '5/clean_Report_BARCLAYS_2018-10-22_Page_1_Doc_1.txt'\n",
    "         , 'random text/very_long_random_financial_text.txt'\n",
    "         , 'random text/achaemenid_empire_history.txt'\n",
    "         ]\n",
    "texts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyst reports = processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read text from all four files\n",
    "files = ['1/KB Home earnings transcript summary.txt'\n",
    "         , '1/KB Home Report Summary.txt'\n",
    "         , '2/McCormic call summary.txt'\n",
    "         , '2/McCormic analyst summary.txt'\n",
    "         , '3/ppg earnings summary.txt'\n",
    "         ,'3/ppg analyst summary.txt'\n",
    "         , '4/lenox earnings summary.txt'\n",
    "         ,'4/lenox analyst summary.txt'\n",
    "         , '5/IQVIA earnings summary.txt'\n",
    "         , '5/IQVIA Analyst summary.txt'\n",
    "         , 'random text/very_long_random_financial_text.txt'\n",
    "         \n",
    "         ]\n",
    "texts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyst reports = raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read text from all four files\n",
    "files = ['1/KB Home earnings transcript summary.txt'\n",
    "         , '1/Report_JPMORGAN_2009-09-25_Page_2_Doc_3.txt'\n",
    "         , '2/McCormic call summary.txt'\n",
    "         , '2/Report_JPMORGAN_2009-09-25_Page_1_Doc_37.txt'\n",
    "         , '3/ppg earnings summary.txt'\n",
    "         ,'3/Report_BARCLAYS_2018-10-22_Page_1_Doc_17.txt'\n",
    "         , '4/lenox earnings summary.txt'\n",
    "         ,'4/Report_BARCLAYS_2018-10-22_Page_1_Doc_11.txt'\n",
    "         , '5/IQVIA earnings summary.txt'\n",
    "         , '5/Report_BARCLAYS_2018-10-22_Page_1_Doc_1.txt'\n",
    "         , 'random text/very_long_random_financial_text.txt'\n",
    "         \n",
    "         ]\n",
    "texts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new clean file + claude files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read text from all four files\n",
    "files = ['1/KB Home earnings transcript summary.txt'\n",
    "         ,'1/claude_earnings_calls_KBH.txt'\n",
    "         , '1/clean_Report_JPMORGAN_2009-09-25_Page_2_Doc_3.txt'\n",
    "         , '1/claude_analyst_report.txt'\n",
    "         , '2/McCormic call summary.txt'\n",
    "         , '2/claude_earnings_call_mccormick.txt'\n",
    "         , '2/clean_Report_JPMORGAN_2009-09-25_Page_1_Doc_37.txt'\n",
    "         , '2/claude_clean_Report_JPMORGAN_2009-09-25_Page_1_Doc_37.txt'\n",
    "         , '3/ppg earnings summary.txt'\n",
    "         ,'3/Report_BARCLAYS_2018-10-22_Page_1_Doc_17.txt'\n",
    "         , '4/lenox earnings summary.txt'\n",
    "         ,'4/Report_BARCLAYS_2018-10-22_Page_1_Doc_11.txt'\n",
    "         , '5/IQVIA earnings summary.txt'\n",
    "         , '5/Report_BARCLAYS_2018-10-22_Page_1_Doc_1.txt'\n",
    "         , 'random text/very_long_random_financial_text.txt'\n",
    "         \n",
    "         ]\n",
    "texts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.         0.54340741 0.57424612 0.4133031  0.47723004 0.45362913\n",
      "  0.48966945]\n",
      " [0.54340741 1.         0.54263341 0.44851377 0.45552775 0.48138289\n",
      "  0.43691842]\n",
      " [0.57424612 0.54263341 1.         0.52558972 0.60273339 0.51510664\n",
      "  0.55227756]\n",
      " [0.4133031  0.44851377 0.52558972 1.         0.40243005 0.45437141\n",
      "  0.42070361]\n",
      " [0.47723004 0.45552775 0.60273339 0.40243005 1.         0.62240232\n",
      "  0.52604706]\n",
      " [0.45362913 0.48138289 0.51510664 0.45437141 0.62240232 1.\n",
      "  0.47074733]\n",
      " [0.48966945 0.43691842 0.55227756 0.42070361 0.52604706 0.47074733\n",
      "  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Convert the text to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Step 3: Compute the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Step 4: Display the similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_sim_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.78373633 0.80503697 0.69520024 0.71603061 0.72818072\n",
      "  0.75096994]\n",
      " [0.78373633 1.         0.76697988 0.71582984 0.69607307 0.73573826\n",
      "  0.67899861]\n",
      " [0.80503697 0.76697988 1.         0.72446728 0.80887688 0.7532369\n",
      "  0.77764412]\n",
      " [0.69520024 0.71582984 0.72446728 1.         0.6436006  0.71066632\n",
      "  0.67369463]\n",
      " [0.71603061 0.69607307 0.80887688 0.6436006  1.         0.7675998\n",
      "  0.74524009]\n",
      " [0.72818072 0.73573826 0.7532369  0.71066632 0.7675998  1.\n",
      "  0.72056979]\n",
      " [0.75096994 0.67899861 0.77764412 0.67369463 0.74524009 0.72056979\n",
      "  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convert texts to Bag of Words vectors\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cos_sim = cosine_similarity(bow_matrix)\n",
    "print(cos_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.29726947 0.23183105 0.09723358 0.16898831 0.13568952\n",
      "  0.15200628]\n",
      " [0.29726947 1.         0.22564361 0.17007828 0.15957312 0.19496859\n",
      "  0.12244972]\n",
      " [0.23183105 0.22564361 1.         0.30538522 0.31230233 0.17982566\n",
      "  0.20157331]\n",
      " [0.09723358 0.17007828 0.30538522 1.         0.14424305 0.18466161\n",
      "  0.1156476 ]\n",
      " [0.16898831 0.15957312 0.31230233 0.14424305 1.         0.48181896\n",
      "  0.21683504]\n",
      " [0.13568952 0.19496859 0.17982566 0.18466161 0.48181896 1.\n",
      "  0.20035648]\n",
      " [0.15200628 0.12244972 0.20157331 0.1156476  0.21683504 0.20035648\n",
      "  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess the text (lemmatization)\n",
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Preprocess texts\n",
    "processed_texts = [preprocess(text) for text in texts]\n",
    "\n",
    "# Convert processed texts to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cos_sim = cosine_similarity(tfidf_matrix)\n",
    "print(cos_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using transformers Library (BERT, GPT, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000002  0.8564699  0.64499223 0.80988264 0.5765875  0.52587336\n",
      "  0.5640128  0.4917363  0.47404644 0.46166116 0.47947595 0.51718235\n",
      "  0.49632868 0.51254845 0.40987042]\n",
      " [0.8564699  1.0000002  0.61561716 0.776796   0.45843738 0.46364862\n",
      "  0.47657636 0.41077176 0.3807938  0.39560297 0.4312372  0.5133324\n",
      "  0.35768628 0.40492356 0.32133654]\n",
      " [0.64499223 0.61561716 0.9999999  0.85057116 0.507925   0.5142173\n",
      "  0.6595469  0.49596488 0.47177842 0.6182276  0.46436235 0.6020144\n",
      "  0.41777974 0.53882986 0.43784958]\n",
      " [0.80988264 0.776796   0.85057116 0.9999995  0.5233258  0.55130893\n",
      "  0.639691   0.5327645  0.4582293  0.5815066  0.46017388 0.58348095\n",
      "  0.4980946  0.5743005  0.44902092]\n",
      " [0.5765875  0.45843738 0.507925   0.5233258  1.0000002  0.7459167\n",
      "  0.6322724  0.6057614  0.6167301  0.5096253  0.48556116 0.43228775\n",
      "  0.5148796  0.44470435 0.448818  ]\n",
      " [0.52587336 0.46364862 0.5142173  0.55130893 0.7459167  0.9999992\n",
      "  0.7281099  0.7685079  0.5249032  0.526742   0.4313272  0.52421314\n",
      "  0.43826917 0.46293616 0.43522364]\n",
      " [0.5640128  0.47657636 0.6595469  0.639691   0.6322724  0.7281099\n",
      "  1.0000005  0.82891077 0.50583076 0.63284606 0.49660486 0.58486843\n",
      "  0.4447843  0.60414237 0.42024943]\n",
      " [0.4917363  0.41077176 0.49596488 0.5327645  0.6057614  0.7685079\n",
      "  0.82891077 1.0000001  0.48751625 0.5194444  0.44038984 0.5136644\n",
      "  0.37205562 0.5387716  0.41419476]\n",
      " [0.47404644 0.3807938  0.47177842 0.4582293  0.6167301  0.5249032\n",
      "  0.50583076 0.48751625 1.         0.7226618  0.46227488 0.5120401\n",
      "  0.45631278 0.42961892 0.55043924]\n",
      " [0.46166116 0.39560297 0.6182276  0.5815066  0.5096253  0.526742\n",
      "  0.63284606 0.5194444  0.7226618  0.9999996  0.49744684 0.63623834\n",
      "  0.4506098  0.51751494 0.5763457 ]\n",
      " [0.47947595 0.4312372  0.46436235 0.46017388 0.48556116 0.4313272\n",
      "  0.49660486 0.44038984 0.46227488 0.49744684 1.0000001  0.65030134\n",
      "  0.38297    0.435541   0.42238602]\n",
      " [0.51718235 0.5133324  0.6020144  0.58348095 0.43228775 0.52421314\n",
      "  0.58486843 0.5136644  0.5120401  0.63623834 0.65030134 0.9999997\n",
      "  0.40438217 0.5085432  0.5239339 ]\n",
      " [0.49632868 0.35768628 0.41777974 0.4980946  0.5148796  0.43826917\n",
      "  0.4447843  0.37205562 0.45631278 0.4506098  0.38297    0.40438217\n",
      "  1.0000004  0.8168337  0.50211835]\n",
      " [0.51254845 0.40492356 0.53882986 0.5743005  0.44470435 0.46293616\n",
      "  0.60414237 0.5387716  0.42961892 0.51751494 0.435541   0.5085432\n",
      "  0.8168337  1.0000001  0.48739824]\n",
      " [0.40987042 0.32133654 0.43784958 0.44902092 0.448818   0.43522364\n",
      "  0.42024943 0.41419476 0.55043924 0.5763457  0.42238602 0.5239339\n",
      "  0.50211835 0.48739824 0.9999998 ]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Load a free pre-trained model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  # Free and publicly available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: Function to compute embeddings for a text\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Step 3: Load text files and compute embeddings\n",
    "texts = []\n",
    "embeddings = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "        text = f.read()\n",
    "        texts.append(text)\n",
    "        embeddings.append(get_embeddings(text))\n",
    "\n",
    "# Step 4: Compute cosine similarity\n",
    "cosine_sim = cosine_similarity(embeddings)\n",
    "print(cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.7790, 0.4919, 0.6715, 0.5429, 0.4687, 0.4763, 0.4284, 0.4576,\n",
      "         0.3428, 0.3925, 0.4039, 0.4194, 0.3449, 0.3113],\n",
      "        [0.7790, 1.0000, 0.5459, 0.7403, 0.4178, 0.3939, 0.3815, 0.3294, 0.3335,\n",
      "         0.2989, 0.3352, 0.4262, 0.3047, 0.3381, 0.2063],\n",
      "        [0.4919, 0.5459, 1.0000, 0.8016, 0.4069, 0.4257, 0.4619, 0.3800, 0.4024,\n",
      "         0.4459, 0.3339, 0.4817, 0.3068, 0.4022, 0.3146],\n",
      "        [0.6715, 0.7403, 0.8016, 1.0000, 0.4363, 0.4647, 0.4673, 0.3946, 0.4006,\n",
      "         0.4232, 0.3382, 0.4569, 0.4022, 0.4060, 0.3625],\n",
      "        [0.5429, 0.4178, 0.4069, 0.4363, 1.0000, 0.7806, 0.6183, 0.6639, 0.5264,\n",
      "         0.4377, 0.4776, 0.3239, 0.4892, 0.3795, 0.3056],\n",
      "        [0.4687, 0.3939, 0.4257, 0.4647, 0.7806, 1.0000, 0.7131, 0.7702, 0.5080,\n",
      "         0.4535, 0.3670, 0.3884, 0.3760, 0.3417, 0.3048],\n",
      "        [0.4763, 0.3815, 0.4619, 0.4673, 0.6183, 0.7131, 1.0000, 0.8160, 0.4924,\n",
      "         0.5182, 0.3896, 0.4232, 0.3963, 0.4536, 0.3297],\n",
      "        [0.4284, 0.3294, 0.3800, 0.3946, 0.6639, 0.7702, 0.8160, 1.0000, 0.4228,\n",
      "         0.4404, 0.3843, 0.3344, 0.3264, 0.4366, 0.2398],\n",
      "        [0.4576, 0.3335, 0.4024, 0.4006, 0.5264, 0.5080, 0.4924, 0.4228, 1.0000,\n",
      "         0.7449, 0.4582, 0.4600, 0.4859, 0.3735, 0.5588],\n",
      "        [0.3428, 0.2989, 0.4459, 0.4232, 0.4377, 0.4535, 0.5182, 0.4404, 0.7449,\n",
      "         1.0000, 0.4325, 0.5347, 0.4107, 0.3899, 0.5340],\n",
      "        [0.3925, 0.3352, 0.3339, 0.3382, 0.4776, 0.3670, 0.3896, 0.3843, 0.4582,\n",
      "         0.4325, 1.0000, 0.5696, 0.4214, 0.3585, 0.3280],\n",
      "        [0.4039, 0.4262, 0.4817, 0.4569, 0.3239, 0.3884, 0.4232, 0.3344, 0.4600,\n",
      "         0.5347, 0.5696, 1.0000, 0.3726, 0.3479, 0.5183],\n",
      "        [0.4194, 0.3047, 0.3068, 0.4022, 0.4892, 0.3760, 0.3963, 0.3264, 0.4859,\n",
      "         0.4107, 0.4214, 0.3726, 1.0000, 0.7435, 0.5017],\n",
      "        [0.3449, 0.3381, 0.4022, 0.4060, 0.3795, 0.3417, 0.4536, 0.4366, 0.3735,\n",
      "         0.3899, 0.3585, 0.3479, 0.7435, 1.0000, 0.3917],\n",
      "        [0.3113, 0.2063, 0.3146, 0.3625, 0.3056, 0.3048, 0.3297, 0.2398, 0.5588,\n",
      "         0.5340, 0.3280, 0.5183, 0.5017, 0.3917, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Step 1: Load a free pre-trained sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 2: Load text files and compute embeddings\n",
    "texts = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "        texts.append(f.read())\n",
    "\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Step 3: Compute cosine similarity using sentence-transformers utility function\n",
    "cosine_sim = util.cos_sim(embeddings, embeddings)\n",
    "print(cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings from voyage AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0164]])\n"
     ]
    }
   ],
   "source": [
    "import voyageai\n",
    "from sentence_transformers import util\n",
    "api_key = \"pa-9RM2LNXb5nO0badzm9pyyAnmxIajhP3kH_J-sXt7JXY\"\n",
    "vo = voyageai.Client(api_key=api_key)\n",
    "# This will automatically use the environment variable VOYAGE_API_KEY.\n",
    "# Alternatively, you can use vo = voyageai.Client(api_key=\"<your secret key>\")\n",
    "\n",
    "texts_for_embeddings = [texts[5],texts[20]]\n",
    "\n",
    "result = vo.embed(texts_for_embeddings, model=\"voyage-finance-2\", input_type=\"document\", truncation=False)\n",
    "cosine_sim = util.cos_sim(result.embeddings[0], result.embeddings[1])\n",
    "print(cosine_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
