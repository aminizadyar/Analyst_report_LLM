{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not-summarized earnings calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read text from all four files\n",
    "files = ['1/KB Home, Q3 2009 Earnings Call, Sep-25-2009.txt'\n",
    "         ,'1/KB Home earnings transcript summary.txt' \n",
    "         , '1/claude_earnings_calls_KBH.txt'\n",
    "         , '1/clean_Report_JPMORGAN_2009-09-25_Page_2_Doc_3.txt'\n",
    "         , '1/claude_analyst_report.txt'\n",
    "         , '2/McCormick & Co. Inc., Q3 2009 Earnings Call, Sep-24-2009 (1).txt'\n",
    "         , '2/McCormic call summary.txt'\n",
    "         , '2/claude_earnings_call_mccormick.txt'\n",
    "         , '2/clean_Report_JPMORGAN_2009-09-25_Page_1_Doc_37.txt'\n",
    "         , '2/claude_clean_Report_JPMORGAN_2009-09-25_Page_1_Doc_37.txt'\n",
    "         , '3/PPG Industries, Inc., Q3 2018 Earnings Call, Oct 18, 2018.txt'\n",
    "         , '3/ppg earnings summary.txt'\n",
    "         , '3/clean_Report_BARCLAYS_2018-10-22_Page_1_Doc_17.txt'\n",
    "         , '4/Lennox International Inc., Q3 2018 Earnings Call, Oct 22, 2018.txt'\n",
    "         , '4/lenox earnings summary.txt'\n",
    "         , '4/clean_Report_BARCLAYS_2018-10-22_Page_1_Doc_11.txt'\n",
    "         , '5/IQVIA Holdings Inc., Q3 2018 Earnings Call, Oct 22, 2018.txt'\n",
    "         , '5/IQVIA earnings summary.txt'\n",
    "         , '5/clean_Report_BARCLAYS_2018-10-22_Page_1_Doc_1.txt'\n",
    "         , 'random text/very_long_random_financial_text.txt'\n",
    "         , 'random text/achaemenid_empire_history.txt'\n",
    "         ]\n",
    "texts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0,3,5,8,10,12,13,15,16,18,19,20]\n",
    "subset = [texts[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyst reports = processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read text from all four files\n",
    "files = ['1/KB Home earnings transcript summary.txt'\n",
    "         , '1/KB Home Report Summary.txt'\n",
    "         , '2/McCormic call summary.txt'\n",
    "         , '2/McCormic analyst summary.txt'\n",
    "         , '3/ppg earnings summary.txt'\n",
    "         ,'3/ppg analyst summary.txt'\n",
    "         , '4/lenox earnings summary.txt'\n",
    "         ,'4/lenox analyst summary.txt'\n",
    "         , '5/IQVIA earnings summary.txt'\n",
    "         , '5/IQVIA Analyst summary.txt'\n",
    "         , 'random text/very_long_random_financial_text.txt'\n",
    "         \n",
    "         ]\n",
    "texts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyst reports = raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read text from all four files\n",
    "files = ['1/KB Home earnings transcript summary.txt'\n",
    "         , '1/Report_JPMORGAN_2009-09-25_Page_2_Doc_3.txt'\n",
    "         , '2/McCormic call summary.txt'\n",
    "         , '2/Report_JPMORGAN_2009-09-25_Page_1_Doc_37.txt'\n",
    "         , '3/ppg earnings summary.txt'\n",
    "         ,'3/Report_BARCLAYS_2018-10-22_Page_1_Doc_17.txt'\n",
    "         , '4/lenox earnings summary.txt'\n",
    "         ,'4/Report_BARCLAYS_2018-10-22_Page_1_Doc_11.txt'\n",
    "         , '5/IQVIA earnings summary.txt'\n",
    "         , '5/Report_BARCLAYS_2018-10-22_Page_1_Doc_1.txt'\n",
    "         , 'random text/very_long_random_financial_text.txt'\n",
    "         \n",
    "         ]\n",
    "texts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new clean file + claude files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read text from all four files\n",
    "files = ['1/KB Home earnings transcript summary.txt'\n",
    "         ,'1/claude_earnings_calls_KBH.txt'\n",
    "         , '1/clean_Report_JPMORGAN_2009-09-25_Page_2_Doc_3.txt'\n",
    "         , '1/claude_analyst_report.txt'\n",
    "         , '2/McCormic call summary.txt'\n",
    "         , '2/claude_earnings_call_mccormick.txt'\n",
    "         , '2/clean_Report_JPMORGAN_2009-09-25_Page_1_Doc_37.txt'\n",
    "         , '2/claude_clean_Report_JPMORGAN_2009-09-25_Page_1_Doc_37.txt'\n",
    "         , '3/ppg earnings summary.txt'\n",
    "         ,'3/Report_BARCLAYS_2018-10-22_Page_1_Doc_17.txt'\n",
    "         , '4/lenox earnings summary.txt'\n",
    "         ,'4/Report_BARCLAYS_2018-10-22_Page_1_Doc_11.txt'\n",
    "         , '5/IQVIA earnings summary.txt'\n",
    "         , '5/Report_BARCLAYS_2018-10-22_Page_1_Doc_1.txt'\n",
    "         , 'random text/very_long_random_financial_text.txt'\n",
    "         \n",
    "         ]\n",
    "texts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.         0.54340741 0.57424612 0.4133031  0.47723004 0.45362913\n",
      "  0.48966945]\n",
      " [0.54340741 1.         0.54263341 0.44851377 0.45552775 0.48138289\n",
      "  0.43691842]\n",
      " [0.57424612 0.54263341 1.         0.52558972 0.60273339 0.51510664\n",
      "  0.55227756]\n",
      " [0.4133031  0.44851377 0.52558972 1.         0.40243005 0.45437141\n",
      "  0.42070361]\n",
      " [0.47723004 0.45552775 0.60273339 0.40243005 1.         0.62240232\n",
      "  0.52604706]\n",
      " [0.45362913 0.48138289 0.51510664 0.45437141 0.62240232 1.\n",
      "  0.47074733]\n",
      " [0.48966945 0.43691842 0.55227756 0.42070361 0.52604706 0.47074733\n",
      "  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Convert the text to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Step 3: Compute the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Step 4: Display the similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_sim_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.78373633 0.80503697 0.69520024 0.71603061 0.72818072\n",
      "  0.75096994]\n",
      " [0.78373633 1.         0.76697988 0.71582984 0.69607307 0.73573826\n",
      "  0.67899861]\n",
      " [0.80503697 0.76697988 1.         0.72446728 0.80887688 0.7532369\n",
      "  0.77764412]\n",
      " [0.69520024 0.71582984 0.72446728 1.         0.6436006  0.71066632\n",
      "  0.67369463]\n",
      " [0.71603061 0.69607307 0.80887688 0.6436006  1.         0.7675998\n",
      "  0.74524009]\n",
      " [0.72818072 0.73573826 0.7532369  0.71066632 0.7675998  1.\n",
      "  0.72056979]\n",
      " [0.75096994 0.67899861 0.77764412 0.67369463 0.74524009 0.72056979\n",
      "  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convert texts to Bag of Words vectors\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cos_sim = cosine_similarity(bow_matrix)\n",
    "print(cos_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.29726947 0.23183105 0.09723358 0.16898831 0.13568952\n",
      "  0.15200628]\n",
      " [0.29726947 1.         0.22564361 0.17007828 0.15957312 0.19496859\n",
      "  0.12244972]\n",
      " [0.23183105 0.22564361 1.         0.30538522 0.31230233 0.17982566\n",
      "  0.20157331]\n",
      " [0.09723358 0.17007828 0.30538522 1.         0.14424305 0.18466161\n",
      "  0.1156476 ]\n",
      " [0.16898831 0.15957312 0.31230233 0.14424305 1.         0.48181896\n",
      "  0.21683504]\n",
      " [0.13568952 0.19496859 0.17982566 0.18466161 0.48181896 1.\n",
      "  0.20035648]\n",
      " [0.15200628 0.12244972 0.20157331 0.1156476  0.21683504 0.20035648\n",
      "  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess the text (lemmatization)\n",
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Preprocess texts\n",
    "processed_texts = [preprocess(text) for text in texts]\n",
    "\n",
    "# Convert processed texts to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cos_sim = cosine_similarity(tfidf_matrix)\n",
    "print(cos_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using transformers Library (BERT, GPT, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000002  0.8564699  0.64499223 0.80988264 0.5765875  0.52587336\n",
      "  0.5640128  0.4917363  0.47404644 0.46166116 0.47947595 0.51718235\n",
      "  0.49632868 0.51254845 0.40987042]\n",
      " [0.8564699  1.0000002  0.61561716 0.776796   0.45843738 0.46364862\n",
      "  0.47657636 0.41077176 0.3807938  0.39560297 0.4312372  0.5133324\n",
      "  0.35768628 0.40492356 0.32133654]\n",
      " [0.64499223 0.61561716 0.9999999  0.85057116 0.507925   0.5142173\n",
      "  0.6595469  0.49596488 0.47177842 0.6182276  0.46436235 0.6020144\n",
      "  0.41777974 0.53882986 0.43784958]\n",
      " [0.80988264 0.776796   0.85057116 0.9999995  0.5233258  0.55130893\n",
      "  0.639691   0.5327645  0.4582293  0.5815066  0.46017388 0.58348095\n",
      "  0.4980946  0.5743005  0.44902092]\n",
      " [0.5765875  0.45843738 0.507925   0.5233258  1.0000002  0.7459167\n",
      "  0.6322724  0.6057614  0.6167301  0.5096253  0.48556116 0.43228775\n",
      "  0.5148796  0.44470435 0.448818  ]\n",
      " [0.52587336 0.46364862 0.5142173  0.55130893 0.7459167  0.9999992\n",
      "  0.7281099  0.7685079  0.5249032  0.526742   0.4313272  0.52421314\n",
      "  0.43826917 0.46293616 0.43522364]\n",
      " [0.5640128  0.47657636 0.6595469  0.639691   0.6322724  0.7281099\n",
      "  1.0000005  0.82891077 0.50583076 0.63284606 0.49660486 0.58486843\n",
      "  0.4447843  0.60414237 0.42024943]\n",
      " [0.4917363  0.41077176 0.49596488 0.5327645  0.6057614  0.7685079\n",
      "  0.82891077 1.0000001  0.48751625 0.5194444  0.44038984 0.5136644\n",
      "  0.37205562 0.5387716  0.41419476]\n",
      " [0.47404644 0.3807938  0.47177842 0.4582293  0.6167301  0.5249032\n",
      "  0.50583076 0.48751625 1.         0.7226618  0.46227488 0.5120401\n",
      "  0.45631278 0.42961892 0.55043924]\n",
      " [0.46166116 0.39560297 0.6182276  0.5815066  0.5096253  0.526742\n",
      "  0.63284606 0.5194444  0.7226618  0.9999996  0.49744684 0.63623834\n",
      "  0.4506098  0.51751494 0.5763457 ]\n",
      " [0.47947595 0.4312372  0.46436235 0.46017388 0.48556116 0.4313272\n",
      "  0.49660486 0.44038984 0.46227488 0.49744684 1.0000001  0.65030134\n",
      "  0.38297    0.435541   0.42238602]\n",
      " [0.51718235 0.5133324  0.6020144  0.58348095 0.43228775 0.52421314\n",
      "  0.58486843 0.5136644  0.5120401  0.63623834 0.65030134 0.9999997\n",
      "  0.40438217 0.5085432  0.5239339 ]\n",
      " [0.49632868 0.35768628 0.41777974 0.4980946  0.5148796  0.43826917\n",
      "  0.4447843  0.37205562 0.45631278 0.4506098  0.38297    0.40438217\n",
      "  1.0000004  0.8168337  0.50211835]\n",
      " [0.51254845 0.40492356 0.53882986 0.5743005  0.44470435 0.46293616\n",
      "  0.60414237 0.5387716  0.42961892 0.51751494 0.435541   0.5085432\n",
      "  0.8168337  1.0000001  0.48739824]\n",
      " [0.40987042 0.32133654 0.43784958 0.44902092 0.448818   0.43522364\n",
      "  0.42024943 0.41419476 0.55043924 0.5763457  0.42238602 0.5239339\n",
      "  0.50211835 0.48739824 0.9999998 ]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Load a free pre-trained model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  # Free and publicly available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: Function to compute embeddings for a text\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Step 3: Load text files and compute embeddings\n",
    "texts = []\n",
    "embeddings = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "        text = f.read()\n",
    "        texts.append(text)\n",
    "        embeddings.append(get_embeddings(text))\n",
    "\n",
    "# Step 4: Compute cosine similarity\n",
    "cosine_sim = cosine_similarity(embeddings)\n",
    "print(cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.7790, 0.4919, 0.6715, 0.5429, 0.4687, 0.4763, 0.4284, 0.4576,\n",
      "         0.3428, 0.3925, 0.4039, 0.4194, 0.3449, 0.3113],\n",
      "        [0.7790, 1.0000, 0.5459, 0.7403, 0.4178, 0.3939, 0.3815, 0.3294, 0.3335,\n",
      "         0.2989, 0.3352, 0.4262, 0.3047, 0.3381, 0.2063],\n",
      "        [0.4919, 0.5459, 1.0000, 0.8016, 0.4069, 0.4257, 0.4619, 0.3800, 0.4024,\n",
      "         0.4459, 0.3339, 0.4817, 0.3068, 0.4022, 0.3146],\n",
      "        [0.6715, 0.7403, 0.8016, 1.0000, 0.4363, 0.4647, 0.4673, 0.3946, 0.4006,\n",
      "         0.4232, 0.3382, 0.4569, 0.4022, 0.4060, 0.3625],\n",
      "        [0.5429, 0.4178, 0.4069, 0.4363, 1.0000, 0.7806, 0.6183, 0.6639, 0.5264,\n",
      "         0.4377, 0.4776, 0.3239, 0.4892, 0.3795, 0.3056],\n",
      "        [0.4687, 0.3939, 0.4257, 0.4647, 0.7806, 1.0000, 0.7131, 0.7702, 0.5080,\n",
      "         0.4535, 0.3670, 0.3884, 0.3760, 0.3417, 0.3048],\n",
      "        [0.4763, 0.3815, 0.4619, 0.4673, 0.6183, 0.7131, 1.0000, 0.8160, 0.4924,\n",
      "         0.5182, 0.3896, 0.4232, 0.3963, 0.4536, 0.3297],\n",
      "        [0.4284, 0.3294, 0.3800, 0.3946, 0.6639, 0.7702, 0.8160, 1.0000, 0.4228,\n",
      "         0.4404, 0.3843, 0.3344, 0.3264, 0.4366, 0.2398],\n",
      "        [0.4576, 0.3335, 0.4024, 0.4006, 0.5264, 0.5080, 0.4924, 0.4228, 1.0000,\n",
      "         0.7449, 0.4582, 0.4600, 0.4859, 0.3735, 0.5588],\n",
      "        [0.3428, 0.2989, 0.4459, 0.4232, 0.4377, 0.4535, 0.5182, 0.4404, 0.7449,\n",
      "         1.0000, 0.4325, 0.5347, 0.4107, 0.3899, 0.5340],\n",
      "        [0.3925, 0.3352, 0.3339, 0.3382, 0.4776, 0.3670, 0.3896, 0.3843, 0.4582,\n",
      "         0.4325, 1.0000, 0.5696, 0.4214, 0.3585, 0.3280],\n",
      "        [0.4039, 0.4262, 0.4817, 0.4569, 0.3239, 0.3884, 0.4232, 0.3344, 0.4600,\n",
      "         0.5347, 0.5696, 1.0000, 0.3726, 0.3479, 0.5183],\n",
      "        [0.4194, 0.3047, 0.3068, 0.4022, 0.4892, 0.3760, 0.3963, 0.3264, 0.4859,\n",
      "         0.4107, 0.4214, 0.3726, 1.0000, 0.7435, 0.5017],\n",
      "        [0.3449, 0.3381, 0.4022, 0.4060, 0.3795, 0.3417, 0.4536, 0.4366, 0.3735,\n",
      "         0.3899, 0.3585, 0.3479, 0.7435, 1.0000, 0.3917],\n",
      "        [0.3113, 0.2063, 0.3146, 0.3625, 0.3056, 0.3048, 0.3297, 0.2398, 0.5588,\n",
      "         0.5340, 0.3280, 0.5183, 0.5017, 0.3917, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Step 1: Load a free pre-trained sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 2: Load text files and compute embeddings\n",
    "texts = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "        texts.append(f.read())\n",
    "\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Step 3: Compute cosine similarity using sentence-transformers utility function\n",
    "cosine_sim = util.cos_sim(embeddings, embeddings)\n",
    "print(cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings from voyage AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011461563408374786\n"
     ]
    }
   ],
   "source": [
    "import voyageai\n",
    "from sentence_transformers import util\n",
    "\n",
    "api_key = \"pa-9RM2LNXb5nO0badzm9pyyAnmxIajhP3kH_J-sXt7JXY\"\n",
    "vo = voyageai.Client(api_key=api_key)\n",
    "\n",
    "# This will automatically use the environment variable VOYAGE_API_KEY.\n",
    "# Alternatively, you can use vo = voyageai.Client(api_key=\"<your secret key>\")\n",
    "\n",
    "def get_similarity(text_1,texts_2):\n",
    "    result = vo.embed([text_1,texts_2], model=\"voyage-finance-2\", input_type=\"document\", truncation=False)\n",
    "    return  util.cos_sim(result.embeddings[0], result.embeddings[1])[0][0].item()  # the output is a tensor. The indexing is to get the item.\n",
    "\n",
    "\n",
    "print(get_similarity(texts[0],texts[20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n",
      "0 9\n",
      "0 10\n",
      "0 11\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "1 7\n",
      "1 8\n",
      "1 9\n",
      "1 10\n",
      "1 11\n",
      "2 2\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "2 6\n",
      "2 7\n",
      "2 8\n",
      "2 9\n",
      "2 10\n",
      "2 11\n",
      "3 3\n",
      "3 4\n",
      "3 5\n",
      "3 6\n",
      "3 7\n",
      "3 8\n",
      "3 9\n",
      "3 10\n",
      "3 11\n",
      "4 4\n",
      "4 5\n",
      "4 6\n",
      "4 7\n",
      "4 8\n",
      "4 9\n",
      "4 10\n",
      "4 11\n",
      "5 5\n",
      "5 6\n",
      "5 7\n",
      "5 8\n",
      "5 9\n",
      "5 10\n",
      "5 11\n",
      "6 6\n",
      "6 7\n",
      "6 8\n",
      "6 9\n",
      "6 10\n",
      "6 11\n",
      "7 7\n",
      "7 8\n",
      "7 9\n",
      "7 10\n",
      "7 11\n",
      "8 8\n",
      "8 9\n",
      "8 10\n",
      "8 11\n",
      "9 9\n",
      "9 10\n",
      "9 11\n",
      "10 10\n",
      "10 11\n",
      "11 11\n",
      "     0         1         2         3         4         5         6         7    \n",
      "0   1.0  0.715625  0.515936  0.316538  0.388844  0.210215  0.401630  0.251079  \\\n",
      "1   NaN  1.000000  0.342347  0.466127  0.252886  0.353695  0.241332  0.313835   \n",
      "2   NaN       NaN  1.000000  0.602182  0.570616  0.345782  0.498446  0.359244   \n",
      "3   NaN       NaN       NaN  1.000000  0.295121  0.420456  0.214342  0.311944   \n",
      "4   NaN       NaN       NaN       NaN  1.000000  0.595137  0.614676  0.407884   \n",
      "5   NaN       NaN       NaN       NaN       NaN  1.000000  0.334343  0.447622   \n",
      "6   NaN       NaN       NaN       NaN       NaN       NaN  1.000000  0.672579   \n",
      "7   NaN       NaN       NaN       NaN       NaN       NaN       NaN  1.000000   \n",
      "8   NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "9   NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "10  NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "11  NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "          8         9         10        11  \n",
      "0   0.345995  0.220249  0.137900  0.001146  \n",
      "1   0.222665  0.366236  0.112483 -0.016029  \n",
      "2   0.370349  0.211083  0.221107  0.016425  \n",
      "3   0.194629  0.309201  0.157369 -0.047866  \n",
      "4   0.481524  0.240630  0.233899 -0.045840  \n",
      "5   0.319578  0.451080  0.219554 -0.012030  \n",
      "6   0.483768  0.290813  0.138426 -0.083043  \n",
      "7   0.328731  0.398820  0.080841 -0.014669  \n",
      "8   1.000000  0.681117  0.141805 -0.013295  \n",
      "9        NaN  1.000000  0.115930  0.010394  \n",
      "10       NaN       NaN  1.000000  0.067593  \n",
      "11       NaN       NaN       NaN  1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame with subset as both row and column labels\n",
    "similarity_matrix = pd.DataFrame()\n",
    "\n",
    "# Loop over each pair and calculate the similarity\n",
    "counter = len(subset)\n",
    "for i in range(counter):\n",
    "    for j in range(i,counter):\n",
    "        print(i,j)\n",
    "        similarity_matrix.loc[i, j] = get_similarity(subset[i], subset[j])\n",
    "\n",
    "# Convert the matrix to float or integer type for numerical operations if needed\n",
    "similarity_matrix = similarity_matrix.astype(float)\n",
    "\n",
    "# Print or visualize the similarity matrix\n",
    "print(similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
